{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "887e574c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    RANDOM_STATE_SEED = 42\n",
    "    MODEL_LIST = [\n",
    "        # \"japanese-asr/distil-whisper-large-v3-ja-reazonspeech-all\",\n",
    "        # \"openai/whisper-tiny\",\n",
    "        # \"openai/whisper-small\",\n",
    "        # \"openai/whisper-medium\",\n",
    "        # \"openai/whisper-large-v3\",\n",
    "        \"./whisper-ja-asmr-tiny-1-earlyst/final\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73c49b0f",
   "metadata": {
    "title": "1) Load test split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1698 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rehab/research/whisperlaz/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./manifest/preprocessed-segments-index.csv\")\n",
    "df = df[df.lang == \"ja\"].reset_index(drop=True)\n",
    "_, test_df = np.split(df.sample(frac=1, random_state=Config.RANDOM_STATE_SEED),\n",
    "                      [int(len(df)*0.9)])\n",
    "print(f\"Loaded {len(test_df)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29018d5c",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "2) Metrics & device"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4de0244",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "3) Helper to evaluate one model"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_name: str):\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model     = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens      = []\n",
    "\n",
    "    preds, refs = [], []\n",
    "    for _, row in tqdm(test_df.iterrows(),\n",
    "                       total=len(test_df),\n",
    "                       desc=f\"Eval {model_name}\"):\n",
    "        data = np.load(row[\"npz_path\"], allow_pickle=True)\n",
    "        audio_array = data[\"audio\"].astype(np.float32)  # raw waveform\n",
    "        transcript  = str(data[\"text\"])\n",
    "\n",
    "        # *** CORRECTED HERE ***\n",
    "        inputs = processor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                inputs,\n",
    "                max_length=256,\n",
    "                num_beams=1,\n",
    "            )\n",
    "        pred = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append(transcript)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=preds, references=refs)\n",
    "    cer = cer_metric.compute(predictions=preds, references=refs)\n",
    "    return wer, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "060e43eb",
   "metadata": {
    "title": "4) Run through your models"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c16efd98f9421d88d67960e2c79d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd96e150936841f7ab71c8febfcf2846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ./whisper-ja-asmr-tiny-1-earlyst/final:   0%|          | 0/1698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m results = {}\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tqdm(Config.MODEL_LIST):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     wer, cer = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     results[m] = {\u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m: wer, \u001b[33m\"\u001b[39m\u001b[33mcer\u001b[39m\u001b[33m\"\u001b[39m: cer}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m     16\u001b[39m inputs = processor(\n\u001b[32m     17\u001b[39m     audio_array,\n\u001b[32m     18\u001b[39m     sampling_rate=\u001b[32m16000\u001b[39m,\n\u001b[32m     19\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m ).input_features.to(device)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m pred = processor.batch_decode(\n\u001b[32m     29\u001b[39m     generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     30\u001b[39m )[\u001b[32m0\u001b[39m]\n\u001b[32m     32\u001b[39m preds.append(pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/whisperlaz/.venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/whisperlaz/.venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/whisperlaz/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/whisperlaz/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2358\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2347\u001b[39m     warnings.warn(\n\u001b[32m   2348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m than your model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, whereas the model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2354\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2355\u001b[39m     )\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# 9. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2358\u001b[39m prepared_logits_processor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2369\u001b[39m prepared_stopping_criteria = \u001b[38;5;28mself\u001b[39m._get_stopping_criteria(\n\u001b[32m   2370\u001b[39m     generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n\u001b[32m   2371\u001b[39m )\n\u001b[32m   2373\u001b[39m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/whisperlaz/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1169\u001b[39m, in \u001b[36mGenerationMixin._get_logits_processor\u001b[39m\u001b[34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[39m\n\u001b[32m   1160\u001b[39m     processors.append(\n\u001b[32m   1161\u001b[39m         SuppressTokensAtBeginLogitsProcessor(\n\u001b[32m   1162\u001b[39m             generation_config.begin_suppress_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1165\u001b[39m         )\n\u001b[32m   1166\u001b[39m     )\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.forced_decoder_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1168\u001b[39m     \u001b[38;5;66;03m# TODO (sanchit): move this exception to GenerationConfig.validate() when TF & FLAX are aligned with PT\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min favour of `input_ids` or `decoder_input_ids` respectively.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1172\u001b[39m     )\n\u001b[32m   1174\u001b[39m \u001b[38;5;66;03m# TODO (joao): find a strategy to specify the order of the processors\u001b[39;00m\n\u001b[32m   1175\u001b[39m processors = \u001b[38;5;28mself\u001b[39m._merge_criteria_processor_list(processors, logits_processor)\n",
      "\u001b[31mValueError\u001b[39m: You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively."
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for m in tqdm(Config.MODEL_LIST):\n",
    "    wer, cer = evaluate_model(m)\n",
    "    results[m] = {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbae98c",
   "metadata": {
    "title": "5) Build a pandas DataFrame"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>wer</th>\n",
       "      <th>cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japanese-asr/distil-whisper-large-v3-ja-reazon...</td>\n",
       "      <td>0.905581</td>\n",
       "      <td>0.283509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai/whisper-tiny</td>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.127423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>1.350399</td>\n",
       "      <td>0.528739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openai/whisper-medium</td>\n",
       "      <td>1.198070</td>\n",
       "      <td>0.560072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai/whisper-large-v3</td>\n",
       "      <td>0.966429</td>\n",
       "      <td>0.310716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model       wer       cer\n",
       "0  japanese-asr/distil-whisper-large-v3-ja-reazon...  0.905581  0.283509\n",
       "1                                openai/whisper-tiny  1.581200  1.127423\n",
       "2                               openai/whisper-small  1.350399  0.528739\n",
       "3                              openai/whisper-medium  1.198070  0.560072\n",
       "4                            openai/whisper-large-v3  0.966429  0.310716"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame([\n",
    "    {\"model\": model_name, \"wer\": metrics[\"wer\"], \"cer\": metrics[\"cer\"]}\n",
    "    for model_name, metrics in results.items()\n",
    "])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d855015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"./manifest/model-list-evals.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
